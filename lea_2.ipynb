{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2eac4ab-bb1a-40ff-bd10-eff61bc87917",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import import_dataset_fct\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660a4f1a-2b2f-4773-bc53-753a916412d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import_dataset_fct.return_filenames()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdecc5d4-0fb0-4b77-b303-4173723f8a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = import_dataset_fct.get_file(\"Dataset_1_train_asset.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6014f7e1-afcd-44b1-adc3-f35f8da42c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def col_numeric_names(df):\n",
    "    df = df.rename(columns={x:y for x,y in zip(df.columns,range(0,len(df.columns)))})\n",
    "    return df\n",
    "\n",
    "def absolute_growth(df):\n",
    "    len_df = df.shape[1]\n",
    "    temp = df.iloc[:,1:len_df]\n",
    "    temp = col_numeric_names(temp)\n",
    "    temp2 = df.iloc[:,:len_df-1]\n",
    "    df2 = temp-temp2\n",
    "    df2.insert(0, \"0\", 0)\n",
    "    df2 = col_numeric_names(df2)\n",
    "    return df2\n",
    "df_absolute_growth = absolute_growth(df)\n",
    "df_absolute_growth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bebcded-795d-4811-81dc-1ffc33814cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(S):\n",
    "    mean = S.mean(axis=1)\n",
    "    std = S.std(axis=1)\n",
    "    keys = np.arange(0, 30, 1)\n",
    "    S_std = pd.concat([std]*len(keys), keys=keys, axis=1)\n",
    "    S_mean = pd.concat([mean]*len(keys), keys=keys, axis=1)\n",
    "    S_stand = (S - S_mean)/S_std\n",
    "    #S_stand = S/S_std\n",
    "    return S_stand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c49202a-68db-4298-a382-6725473312c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Set up device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db8014c-6737-4522-b847-9a17f1ce07c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def col_numeric_names(df):\n",
    "    df = df.rename(columns={x:y for x,y in zip(df.columns,range(0,len(df.columns)))})\n",
    "    return df\n",
    "\n",
    "def absolute_growth(df):\n",
    "    len_df = df.shape[1]\n",
    "    temp = df.iloc[:,1:len_df]\n",
    "    temp = col_numeric_names(temp)\n",
    "    temp2 = df.iloc[:,:len_df-1]\n",
    "    df2 = temp-temp2\n",
    "    df2.insert(0, \"0\", 0)\n",
    "    df2 = col_numeric_names(df2)\n",
    "    return df2\n",
    "\n",
    "def get_train_val(url_data, url_payoff, split_percent=0.8):\n",
    "    df = import_dataset_fct.get_file(url_data)\n",
    "    pay_off = pd.read_csv(url_payoff, header=None)\n",
    "    df_absolute_growth = absolute_growth(df)\n",
    "    df = standardize(df.iloc[:,:30])\n",
    "    data = np.array(df.values.astype('float32'))\n",
    "    #scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    #data = scaler.fit_transform(data).flatten()\n",
    "    pay_off = np.array(pay_off)\n",
    "    n = len(data)\n",
    "    # Point for splitting data into train and test\n",
    "    split = int(n*split_percent)\n",
    "    train_data = data[range(split)]\n",
    "    val_data = data[split:]\n",
    "    n = len(df)\n",
    "    split = int(n*split_percent)\n",
    "    train_payoff = pay_off[range(split)]\n",
    "    train_growth = np.array(df_absolute_growth.iloc[0:split, 1:])\n",
    "    #train_growth = np.array(df_absolute_growth.iloc[0:split, :30])\n",
    "    val_payoff = pay_off[split:]\n",
    "    val_growth = np.array(df_absolute_growth.iloc[split:, 1:])\n",
    "    #val_growth = np.array(df_absolute_growth.iloc[split:, :30])\n",
    "    return train_data, val_data, train_payoff, val_payoff, train_growth, val_growth, data\n",
    "\n",
    "url_payoff = \"/home/jovyan/hfactory_magic_folders/natixis_data_challenge_22_23/erm/Dataset_1_train_payoff.csv\"\n",
    "url_data = \"Dataset_1_train_asset.csv\"\n",
    "train_data, val_data, train_payoff, val_payoff, train_growth, val_growth, data = get_train_val(url_data, url_payoff, split_percent=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd83a84f-0f7c-47bc-abe9-3ed523d9c11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(list(torch.utils.data.TensorDataset(torch.Tensor(train_growth), torch.Tensor(train_payoff))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b7a402-8e6e-4711-8591-4be6ba821692",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(train_data, val_data, train_payoff, val_payoff, train_growth, val_growth, batch_size):\n",
    "    train_payoff = torch.Tensor(train_payoff)\n",
    "    val_payoff = torch.Tensor(val_payoff)\n",
    "    train_growth = torch.Tensor(train_growth)\n",
    "    val_growth = torch.Tensor(val_growth)\n",
    "    train_X = torch.Tensor(train_data)\n",
    "    train_transaction_costs = torch.Tensor([0.0] * train_X.shape[0])\n",
    "    val_X = torch.Tensor(val_data)\n",
    "    val_transaction_costs = torch.Tensor([0.0] * val_X.shape[0])\n",
    "    dataset_train = torch.utils.data.TensorDataset(train_X, train_payoff, train_growth, train_transaction_costs)\n",
    "    dataset_val = torch.utils.data.TensorDataset(val_X, val_payoff, val_growth, val_transaction_costs)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = torch.utils.data.DataLoader(dataset_val, batch_size=batch_size, shuffle=False)\n",
    "    return dataset_train, dataset_val, train_loader, val_loader\n",
    "\n",
    "batch_size = 256\n",
    "dataset_train, dataset_val, train_loader, val_loader = prepare_data(train_data, val_data, train_payoff, val_payoff, train_growth, val_growth, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306ffcef-d158-4574-bfdc-86968432fbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepHedging_RNN(nn.Module):\n",
    "    def __init__(self, input_size, HL_size, output_size):\n",
    "        super(DeepHedging_RNN, self).__init__()\n",
    "        self.rnn = torch.nn.RNN(input_size=input_size,\n",
    "                                hidden_size=HL_size,\n",
    "                                num_layers=1)\n",
    "        self.linear = torch.nn.Linear(HL_size, output_size)\n",
    "        #self.sigmoid = torch.nn.Sigmoid()\n",
    "    def forward(self, S):\n",
    "        out, _ = self.rnn(S)\n",
    "        out = self.linear(out)\n",
    "        #out = self.sigmoid(out)\n",
    "        return out.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68409ea2-42d4-45f9-b6cd-ee42c38ace38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch.nn.functional as F\n",
    "'''\n",
    "class BS_Loss(torch.nn.Module):\n",
    "    def __init__(self, batch_size, beta=1):\n",
    "        super(BS_Loss, self).__init__()\n",
    "        self.beta = beta\n",
    "        # initialize alpha\n",
    "        self.q1 = nn.Parameter(torch.randn(1), requires_grad=True)\n",
    "        self.q2 = nn.Parameter(torch.randn(1), requires_grad=True)\n",
    "        \n",
    "    def forward(self, deltas, pay_off, growth, costs):\n",
    "        S_delta_T = torch.transpose(growth, 0, 1)\n",
    "        delta_S = torch.diagonal(torch.matmul(deltas, S_delta_T))\n",
    "        loss = torch.sub(pay_off.squeeze(), delta_S)\n",
    "        #es_50 = torch.median(loss) \n",
    "        #es_99 = np.percentile(loss.cpu().detach().numpy(), q=99)\n",
    "        #es_99 = torch.mean(- torch.topk(-loss, math.ceil(0.01*30))[0])\n",
    "        #es_50 = torch.mean(- torch.topk(-loss, math.ceil(0.5*30))[0])\n",
    "        #zeros = torch.zeros(loss.shape[0])\n",
    "        #loss_q1 = torch.max((loss-self.q1), zeros)\n",
    "        es_99 = (F.relu(loss-self.q1).mean())/(1-0.99) + self.q1\n",
    "        #loss_q2 = torch.max((loss-self.q2), zeros)\n",
    "        es_50 = (F.relu(loss-self.q2).mean())/(1-0.5) + self.q2\n",
    "        risk_measure = (es_50 + es_99*self.beta)/(1+self.beta)\n",
    "        return risk_measure\n",
    "'''\n",
    "def loss(deltas, pay_off, growth, costs, q1, q2, beta=1):\n",
    "    S_delta_T = torch.transpose(growth, 0, 1)\n",
    "    delta_S = torch.diagonal(torch.matmul(deltas, S_delta_T))\n",
    "    loss = torch.sub(pay_off.squeeze(), delta_S)\n",
    "    #es_50 = torch.median(loss) \n",
    "    #es_99 = np.percentile(loss.cpu().detach().numpy(), q=99)\n",
    "    #es_99 = torch.mean(- torch.topk(-loss, math.ceil(0.01*30))[0])\n",
    "    #es_50 = torch.mean(- torch.topk(-loss, math.ceil(0.5*30))[0])\n",
    "    #zeros = torch.zeros(loss.shape[0])\n",
    "    #loss_q1 = torch.max((loss-self.q1), zeros)\n",
    "    es_99 = (F.relu(loss-q1).mean())/(1-0.99) + q1\n",
    "    #loss_q2 = torch.max((loss-self.q2), zeros)\n",
    "    es_50 = (F.relu(loss-q2).mean())/(1-0.5) + q2\n",
    "    risk_measure = (es_50 + es_99*beta)/(1+beta)\n",
    "    return risk_measure\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97611b5-1427-46f9-816b-062d619cb195",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "learning_rate = 0.01\n",
    "\n",
    "input_size = 1 # number of expected features\n",
    "output_size = 1\n",
    "HL_size = 16\n",
    "model = DeepHedging_RNN(input_size, HL_size, output_size)\n",
    "# Set up optimizer\n",
    "#q1 = nn.Parameter(torch.randn(1), requires_grad=True)\n",
    "q1 = torch.tensor((0.), dtype=torch.float32, requires_grad=True, device=device)\n",
    "q2 = torch.tensor((0.), dtype=torch.float32, requires_grad=True, device=device)\n",
    "optimizer = optim.Adam(list(model.parameters()) + [q1, q2], lr=learning_rate)\n",
    "#criterion = BS_Loss(batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ed33f3-1392-4fd6-98eb-a944f84bc53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    counter = 0\n",
    "\n",
    "    for train_X, train_payoff, train_growth, costs in train_loader:\n",
    "        counter += 1\n",
    "\n",
    "        # Calculate gradients and update model weights\n",
    "        optimizer.zero_grad()\n",
    "        train_X = train_X.unsqueeze(-1)\n",
    "        deltas = model(train_X)\n",
    "        losses = loss(deltas, train_payoff, train_growth, costs, q1, q2)\n",
    "        running_loss += losses.item()\n",
    "        \n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    epoch_train_loss = running_loss / counter\n",
    "    train_loss.append(epoch_train_loss)\n",
    "    print(\"Train loss: {}\".format(epoch_train_loss))\n",
    "    \n",
    "    running_loss_test = 0.0\n",
    "    counter = 0\n",
    "\n",
    "    for val_X, val_payoff, val_growth, costs in val_loader:\n",
    "        model.eval()\n",
    "        \n",
    "        counter += 1\n",
    "        val_X = val_X.unsqueeze(-1)\n",
    "        deltas = model(val_X)\n",
    "        losses_test = loss(deltas, val_payoff, val_growth, costs, q1, q2)\n",
    "        running_loss_test += losses_test.item()\n",
    "\n",
    "    epoch_test_loss = running_loss_test / counter\n",
    "    val_loss.append(epoch_test_loss)\n",
    "    print(\"Test loss: {}\".format(epoch_test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e5dd43-04b3-4577-9968-6ac7bd01d838",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_eval(url_data, url_payoff):\n",
    "    df = import_dataset_fct.get_file(url_data)\n",
    "    pay_off = pd.read_csv(url_payoff, header=None)\n",
    "    df_absolute_growth = absolute_growth(df).iloc[:,1:]\n",
    "    df = standardize(df.iloc[:,:30])\n",
    "    data = np.array(df.values.astype('float32'))\n",
    "    pay_off = np.array(pay_off)\n",
    "    growth = np.array(df_absolute_growth.values.astype('float32'))\n",
    "    pay_off = torch.Tensor(pay_off)\n",
    "    data = torch.Tensor(data)\n",
    "    growth = torch.Tensor(growth)\n",
    "    return data, pay_off, growth\n",
    "url_payoff = \"/home/jovyan/hfactory_magic_folders/natixis_data_challenge_22_23/erm/Dataset_1_train_payoff.csv\"\n",
    "url_data = \"Dataset_1_train_asset.csv\"\n",
    "data, pay_off, growth = get_data_eval(url_data, url_payoff)\n",
    "data = data.to(device).unsqueeze(-1)\n",
    "predictions = model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606f2504-6593-46e0-b7e5-f8e5f19fcce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(deltas, pay_off, growth, beta=1):\n",
    "    S_delta_T = torch.transpose(growth, 0, 1)\n",
    "    delta_S = torch.diagonal(torch.matmul(deltas, S_delta_T))\n",
    "    loss = torch.sub(pay_off.squeeze(), delta_S)\n",
    "    es_99 = torch.mean(- torch.topk(-loss, math.ceil(0.01*10000))[0])\n",
    "    es_50 = torch.mean(- torch.topk(-loss, math.ceil(0.5*10000))[0])\n",
    "    risk_measure = (es_50 + es_99*beta)/(1+beta)\n",
    "    return risk_measure\n",
    "evaluate(predictions, pay_off, growth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04051882-c43f-44c7-9569-3a797a7f1876",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PATH = \"/home/jovyan/natives_deephedging/model_lea\"\n",
    "#torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4952641-ca08-49fc-b67b-834b064c43a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = torch.load(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662e776d-cadf-47a0-8079-628002dc2694",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_test = \"/home/jovyan/natives_deephedging/data/Dataset_1_test_asset.csv\"\n",
    "test_set = pd.read_csv(path_test, header=None)\n",
    "test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2021d7aa-8441-487b-8dab-2e145ae47ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = np.array(test_set.iloc[:,:30].values.astype('float32'))\n",
    "test_X = torch.Tensor(test_X)\n",
    "transaction_costs_test = torch.Tensor([0.0] * test_X.shape[0])\n",
    "dataset_test = torch.utils.data.TensorDataset(test_X, transaction_costs_test)\n",
    "test_loader = torch.utils.data.DataLoader(dataset_test, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04bd735-8ce1-4a6c-9fec-95c506a522ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_X = np.array(test_set.iloc[:,:30].values.astype('float32'))\n",
    "#test_X = torch.tensor(test_X).unsqueeze(-1)\n",
    "#test_prediction = model(test_X)\n",
    "#test_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b0c897-4ead-4fab-b49b-1a0f7dc7a111",
   "metadata": {},
   "outputs": [],
   "source": [
    "final = []\n",
    "for stock, costs in tqdm(test_loader):\n",
    "    stock = stock.to(device).unsqueeze(-1)\n",
    "    final.append(model(stock).cpu().detach().numpy())\n",
    "final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb3af9a-dac6-4644-b91e-af5ba613d9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NormalizeData(data):\n",
    "    return (data - np.min(data)) / (np.max(data) - np.min(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9270f4-3e40-412a-9f12-998cd2916453",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_figures(rnn_bs: pd.DataFrame, df: pd.DataFrame):\n",
    "    #(deltas_BS: pd.DataFrame, rnn_bs: pd.DataFrame, df: pd.DataFrame): \n",
    "    fig, axs = plt.subplots(2, 3)\n",
    "    fig.set_size_inches(18.5, 10.5)\n",
    "    fig.suptitle('Delta spread at different time intervals', size=45)\n",
    "\n",
    "    #axs[0,0].plot(df.loc[:, 0], deltas_BS.loc[:, 0], 'bo', label='Black-Scholes')\n",
    "    axs[0,0].plot(df.loc[:, 0], NormalizeData(rnn_bs.loc[:, 0]), 'gx', label='Delta-RNN')\n",
    "    axs[0,0].legend()\n",
    "    axs[0,0].set_title('Time 0')\n",
    "    axs[0,0].set_ylim([0, 1])\n",
    "    #axs[0,1].plot(df.loc[:, 1], deltas_BS.loc[:, 1], 'bo', label='Black-Scholes')\n",
    "    axs[0,1].plot(df.loc[:, 1], NormalizeData(rnn_bs.loc[:, 1]), 'gx', label='Delta-RNN')\n",
    "    axs[0,1].legend()\n",
    "    axs[0,1].set_title('Time 1')\n",
    "    #axs[0,2].plot(df.loc[:, 5], deltas_BS.loc[:, 5], 'bo', label='Black-Scholes')\n",
    "    axs[0,2].plot(df.loc[:, 5], NormalizeData(rnn_bs.loc[:, 5]), 'gx', label='Delta-RNN')\n",
    "    axs[0,2].legend()\n",
    "    axs[0,2].set_title('Time 5')\n",
    "    #axs[1,0].plot(df.loc[:, 15], deltas_BS.loc[:, 15], 'bo', label='Black-Scholes')\n",
    "    axs[1,0].plot(df.loc[:, 15], NormalizeData(rnn_bs.loc[:, 15]), 'gx', label='Delta-RNN')\n",
    "    axs[1,0].legend()\n",
    "    axs[1,0].set_title('Time 15')\n",
    "    #axs[1,1].plot(df.loc[:, 25], deltas_BS.loc[:, 25], 'bo', label='Black-Scholes')\n",
    "    axs[1,1].plot(df.loc[:, 25], NormalizeData(rnn_bs.loc[:, 25]), 'gx', label='Delta-RNN')\n",
    "    axs[1,1].legend()\n",
    "    axs[1,1].set_title('Time 25')\n",
    "    #axs[1,2].plot(df.loc[:, 30], deltas_BS.loc[:, 30], 'bo', label='Black-Scholes')\n",
    "    axs[1,2].plot(df.loc[:, 30], NormalizeData(rnn_bs.loc[:, 25]), 'gx', label='Delta-RNN')\n",
    "    axs[1,2].legend()\n",
    "    axs[1,2].set_title('Time 30')\n",
    "\n",
    "    for ax in axs.flat:\n",
    "        ax.set(xlabel='Spot price', ylabel='Delta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331dd507-0d53-4bc4-a1d5-1bf118c7e386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exercise 1 - BlackScholes Model\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "N = norm.cdf\n",
    "\n",
    "def bs_call(K: float, #underlying asset's price\n",
    "            T: float, #time till maturity\n",
    "            S: float = 1.0, # strike price\n",
    "            r: float = 0.0,  # risk-free\n",
    "            sigma: float = 0.158):\n",
    "    \n",
    "    d1 = (np.log(S/K) + (r + 0.5 * sigma**2)*T) / (sigma*np.sqrt(T))\n",
    "    d2 = d1 - sigma * np.sqrt(T)\n",
    "    return S * N(d1) - K * np.exp(-r*T) * N(d2), N(-d1)\n",
    "\n",
    "def bs_put(K: float, #underlying asset's price\n",
    "           T: float, #time till maturity\n",
    "           S: float = 1.0, #strike price\n",
    "           r: float = 0.0, # risk-free\n",
    "           sigma: float = 0.158):\n",
    "    \n",
    "    d1 = (np.log(S/K) + (r + 0.5 * sigma**2)*T) / (sigma*np.sqrt(T))\n",
    "    d2 = d1 - sigma * np.sqrt(T)\n",
    "    return K*np.exp(-r*T)*N(-d2) - S*N(-d1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9e719b-6d39-436d-b1b0-e627a7b581d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_hedge(df: pd.DataFrame):\n",
    "    deltas = pd.DataFrame(index=range(df.shape[0]),columns=range(df.shape[1]))\n",
    "    \n",
    "    for i in tqdm(range(0, df.shape[0])):\n",
    "        short_position = 0.0\n",
    "        for j in range(0, df.shape[1]):\n",
    "            S = 100\n",
    "            initial_price = 100\n",
    "            K = df.loc[i, :].values[j]\n",
    "            T = (31 - j)/365\n",
    "            result, delta = bs_call(K=K, T=T, S=S)\n",
    "            short_position += -result\n",
    "            deltas.loc[i, j] = delta\n",
    "\n",
    "    return deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ca9653-562f-46bd-9a22-b550c24b1c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs_deltas_test = calculate_hedge(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c113ab-e10a-416a-9641-9e3a567741cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_figures(bs_deltas_test, pd.DataFrame(final), test_set)\n",
    "plot_figures(pd.DataFrame(final), test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bc379e-e8c7-43df-a93b-f732d0642e6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
