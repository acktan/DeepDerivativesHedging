{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2eac4ab-bb1a-40ff-bd10-eff61bc87917",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import import_dataset_fct\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660a4f1a-2b2f-4773-bc53-753a916412d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import_dataset_fct.return_filenames()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdecc5d4-0fb0-4b77-b303-4173723f8a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = import_dataset_fct.get_file(\"Dataset_1_train_asset.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6014f7e1-afcd-44b1-adc3-f35f8da42c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def col_numeric_names(df):\n",
    "    df = df.rename(columns={x:y for x,y in zip(df.columns,range(0,len(df.columns)))})\n",
    "    return df\n",
    "\n",
    "def absolute_growth(df):\n",
    "    len_df = df.shape[1]\n",
    "    temp = df.iloc[:,1:len_df]\n",
    "    temp = col_numeric_names(temp)\n",
    "    temp2 = df.iloc[:,:len_df-1]\n",
    "    df2 = temp-temp2\n",
    "    df2.insert(0, \"0\", 0)\n",
    "    df2 = col_numeric_names(df2)\n",
    "    return df2\n",
    "df_absolute_growth = absolute_growth(df)\n",
    "df_absolute_growth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c49202a-68db-4298-a382-6725473312c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Set up device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e085e570-523c-44cf-9d97-28c8bc3357bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = df.iloc[:,:30]\n",
    "payoff_data = pd.read_csv(url_payoff, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db8014c-6737-4522-b847-9a17f1ce07c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def col_numeric_names(df):\n",
    "    df = df.rename(columns={x:y for x,y in zip(df.columns,range(0,len(df.columns)))})\n",
    "    return df\n",
    "\n",
    "def absolute_growth(df):\n",
    "    len_df = df.shape[1]\n",
    "    temp = df.iloc[:,1:len_df]\n",
    "    temp = col_numeric_names(temp)\n",
    "    temp2 = df.iloc[:,:len_df-1]\n",
    "    df2 = temp-temp2\n",
    "    df2.insert(0, \"0\", 0)\n",
    "    df2 = col_numeric_names(df2)\n",
    "    return df2\n",
    "\n",
    "def get_train_val(url_data, url_payoff, split_percent=0.8):\n",
    "    df = import_dataset_fct.get_file(url_data)\n",
    "    pay_off = pd.read_csv(url_payoff, header=None)\n",
    "    df_absolute_growth = absolute_growth(df)\n",
    "    data = np.array(df.iloc[:,:30].values.astype('float32'))\n",
    "    #scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    #data = scaler.fit_transform(data).flatten()\n",
    "    pay_off = np.array(pay_off)\n",
    "    n = len(data)\n",
    "    # Point for splitting data into train and test\n",
    "    split = int(n*split_percent)\n",
    "    train_data = data[range(split)]\n",
    "    val_data = data[split:]\n",
    "    n = len(df)\n",
    "    split = int(n*split_percent)\n",
    "    train_payoff = pay_off[range(split)]\n",
    "    train_growth = np.array(df_absolute_growth.iloc[0:split, 1:])\n",
    "    val_payoff = pay_off[split:]\n",
    "    val_growth = np.array(df_absolute_growth.iloc[split:, 1:])\n",
    "    #train_y = np.append(train_payoff, train_growth, axis=1)\n",
    "    #val_y = np.append(val_payoff, val_growth, axis=1)\n",
    "    return train_data, val_data, train_payoff, val_payoff, train_growth, val_growth, data\n",
    "\n",
    "url_payoff = \"/home/jovyan/hfactory_magic_folders/natixis_data_challenge_22_23/erm/Dataset_1_train_payoff.csv\"\n",
    "url_data = \"Dataset_1_train_asset.csv\"\n",
    "train_data, val_data, train_payoff, val_payoff, train_growth, val_growth, data = get_train_val(url_data, url_payoff, split_percent=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd83a84f-0f7c-47bc-abe9-3ed523d9c11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(list(torch.utils.data.TensorDataset(torch.Tensor(train_growth), torch.Tensor(train_payoff))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b7a402-8e6e-4711-8591-4be6ba821692",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(train_data, val_data, train_payoff, val_payoff, train_growth, val_growth, batch_size):\n",
    "    train_payoff = torch.Tensor(train_payoff)\n",
    "    val_payoff = torch.Tensor(val_payoff)\n",
    "    train_growth = torch.Tensor(train_growth)\n",
    "    val_growth = torch.Tensor(val_growth)\n",
    "    train_X = torch.Tensor(train_data)\n",
    "    val_X = torch.Tensor(val_data)\n",
    "    dataset_train = torch.utils.data.TensorDataset(train_X, train_payoff, train_growth)\n",
    "    dataset_val = torch.utils.data.TensorDataset(val_X, val_payoff, val_growth)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = torch.utils.data.DataLoader(dataset_val, batch_size=batch_size, shuffle=False)\n",
    "    return dataset_train, dataset_val, train_loader, val_loader\n",
    "\n",
    "batch_size = 32\n",
    "dataset_train, dataset_val, train_loader, val_loader = prepare_data(train_data, val_data, train_payoff, val_payoff, train_growth, val_growth, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306ffcef-d158-4574-bfdc-86968432fbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepHedging_RNN(nn.Module):\n",
    "    def __init__(self, input_size, HL_size, output_size):\n",
    "        super(DeepHedging_RNN, self).__init__()\n",
    "        self.rnn = torch.nn.RNN(input_size=input_size,\n",
    "                                hidden_size=HL_size,\n",
    "                                num_layers=1)\n",
    "        self.linear = torch.nn.Linear(HL_size, output_size)\n",
    "    def forward(self, S):\n",
    "        out, _ = self.rnn(S)\n",
    "        out = self.linear(out)\n",
    "        return out.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ecc07d-58bb-4dbe-a5a5-9c9c15cd14a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.Tensor(train_growth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4c3f31-5210-438a-9dcc-d125063dc6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.transpose(torch.Tensor(train_growth), 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c07460-5640-463d-8224-4bec006d8c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_growth.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68409ea2-42d4-45f9-b6cd-ee42c38ace38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(deltas, pay_off, growth):\n",
    "    S_delta_T = torch.transpose(growth, 0, 1)\n",
    "    delta_S = torch.diagonal(torch.matmul(deltas, S_delta_T))\n",
    "    loss = torch.sub(pay_off, delta_S)\n",
    "    worst_one_perc = torch.topk(-loss, 3)\n",
    "    es_99 = torch.mean(- torch.topk(-loss, 3)[0])\n",
    "    es_50 = torch.mean(- torch.topk(-loss, 15)[0])\n",
    "    risk_measure = (es_50 + es_99)/2\n",
    "    return risk_measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a034b6f-e896-4235-bda2-f366fc08c6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97611b5-1427-46f9-816b-062d619cb195",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "learning_rate = 0.1\n",
    "\n",
    "input_size = 1 # number of expected features\n",
    "output_size = 30\n",
    "HL_size = 3\n",
    "model = DeepHedging_RNN(input_size, HL_size, output_size)\n",
    "# Set up optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ed33f3-1392-4fd6-98eb-a944f84bc53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    counter = 0\n",
    "\n",
    "    for train_X, train_payoff, train_growth in train_loader:\n",
    "        counter += 1\n",
    "\n",
    "        # Calculate gradients and update model weights\n",
    "        optimizer.zero_grad()\n",
    "        train_X = train_X.unsqueeze(-1)\n",
    "        deltas = model(train_X)\n",
    "        losses = loss(deltas, train_payoff, train_growth)\n",
    "        running_loss += losses.item()\n",
    "        \n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    epoch_train_loss = running_loss / counter\n",
    "    train_loss.append(epoch_train_loss)\n",
    "    print(\"Train loss: {}\".format(epoch_train_loss))\n",
    "    \n",
    "    running_loss_test = 0.0\n",
    "    counter = 0\n",
    "\n",
    "    for val_X, val_payoff, val_growth in val_loader:\n",
    "        model.eval()\n",
    "        \n",
    "        counter += 1\n",
    "        val_X = val_X.unsqueeze(-1)\n",
    "        deltas = model(val_X)\n",
    "        losses_test = loss(deltas, val_payoff, val_growth)\n",
    "        running_loss_test += losses_test.item()\n",
    "\n",
    "    epoch_test_loss = running_loss_test / counter\n",
    "    val_loss.append(epoch_test_loss)\n",
    "    print(\"Test loss: {}\".format(epoch_test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a369474-f10f-4816-8352-9ce7faf5bc74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce72852f-faac-4552-9346-0dbb736e2efe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
